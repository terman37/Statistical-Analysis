predict(L,new)
pred.w.plim <- predict(L, new, interval = "prediction")
pred.w.clim <- predict(L, new, interval = "confidence")
matplot(new$X, cbind(pred.w.clim, pred.w.plim[,-1]),
lty = c(1,2,2,3,3), type = "l", ylab = "predicted y")
# show expectation of Y at point x0
pred.w.plim <- predict(L,  interval = "prediction")
pred.w.clim <- predict(L,  interval = "confidence")
matplot(cbind(pred.w.clim, pred.w.plim[,-1]),
lty = c(1,2,2,3,3), type = "l", ylab = "predicted y")
pred.w.plim <- predict(L, new, interval = "prediction")
pred.w.clim <- predict(L, new, interval = "confidence")
matplot(cbind(pred.w.clim, pred.w.plim[,-1]),
lty = c(1,2,2,3,3), type = "l", ylab = "predicted y")
matplot(new$X,pred.w.clim,
lty = c(1,2,2,3,3), type = "l", ylab = "predicted y")
matplot(new$X,pred.w.plim,
lty = c(1,2,2,3,3), type = "l", ylab = "predicted y")
plot(X,Y)
matlines(new$X, cbind(pred.w.clim, pred.w.plim[,-1]),
lty = c(1,2,2,3,3), type = "l", ylab = "predicted y")
X= runif(50,-7,7)
Y=5-6*X+rnorm(50,0,20)
plot(X,Y)
L=lm(Y~X)
L
# true coeficients
L$coefficients
# show model on plot
abline(L,col='red')
# predict function
help(predict.lm)
# Yhat of your observations
predict(L)
# Predictions for new values
# careful needs X named for new
new = seq(-3,3,0.1) # no
new = data.frame(X=seq(-20,20,0.5))
predict(L,new)
pred.w.plim <- predict(L, new, interval = "prediction") # for new objects
pred.w.clim <- predict(L, new, interval = "confidence") # for expectation
matplot(new$X, cbind(pred.w.clim, pred.w.plim[,-1]),
lty = c(1,2,2,3,3), type = "l", ylab = "predicted y")
# green and red are for expectation of Y
# other are for Y
pred.w.plim <- predict(L, new, interval = "prediction")
pred.w.clim <- predict(L, new, interval = "confidence")
matplot(new$X,pred.w.clim,
lty = c(1,2,2,3,3), type = "l", ylab = "predicted y")
matplot(new$X,pred.w.plim,
lty = c(1,2,2,3,3), type = "l", ylab = "predicted y")
plot(X,Y)
matlines(new$X, cbind(pred.w.clim, pred.w.plim[,-1]),
lty = c(1,2,2,3,3), type = "l", ylab = "predicted y")
X= runif(50,-7,7)
Y=5-6*X+rnorm(50,0,20)
plot(X,Y)
L=lm(Y~X)
L
# true coeficients
L$coefficients
# show model on plot
abline(L,col='red')
# predict function
help(predict.lm)
# Yhat of your observations
predict(L)
# Predictions for new values
# careful needs X named for new
new = seq(-3,3,0.1) # no
new = data.frame(X=seq(-20,20,0.5))
predict(L,new)
pred.w.plim <- predict(L, new, interval = "prediction") # for new objects
pred.w.clim <- predict(L, new, interval = "confidence") # for expectation
matplot(new$X, cbind(pred.w.clim, pred.w.plim[,-1]),
lty = c(1,2,2,3,3), type = "l", ylab = "predicted y")
# green and red are for expectation of Y
# other are for Y
pred.w.plim <- predict(L, new, interval = "prediction")
pred.w.clim <- predict(L, new, interval = "confidence")
matplot(new$X,pred.w.clim,
lty = c(1,2,2,3,3), type = "l", ylab = "predicted y")
matplot(new$X,pred.w.plim,
lty = c(1,2,2,3,3), type = "l", ylab = "predicted y")
plot(X,Y)
matlines(new$X, cbind(pred.w.clim, pred.w.plim[,-1]),
lty = c(1,2,2,3,3), type = "l", ylab = "predicted y")
A=matrix(0,nrow=50,ncol=4)
A[,1]=rexp(50,0.4)
A[,2]=rnorm(50,3,0.5)
A[,3]=rpois(50,0.8)
A[,4]=runif(50,-3,3)
Y=3+2*A[,1]-5*A[,2]+7*A[,4]+rnorm(50)
L=lm(Y~.,data=as.data.frame(A))
L
summary(L)
X = cbind(rep(1,50),A)
X
#rank of X
install.packages('Matrix')
rankMatrix(X)
A[,3]=rpois(50,0.8)
rankMatrix(X)
#rank of X
install.packages('Matrix')
rankMatrix(X)
install.packages("Matrix")
rankMatrix(X)
Matrix.rankMatrix(X)
rankMatrix(X)
# Check details
summary(L)
rankMatrix(X)
# Generate multiple variables
A=matrix(0,nrow=50,ncol=4)
A[,1]=rexp(50,0.4)
A[,2]=rnorm(50,3,0.5)
A[,3]=rpois(50,0.8)
A[,4]=runif(50,-3,3)
# and a response
Y=3+2*A[,1]-5*A[,2]+7*A[,4]+rnorm(50)
# Model
L=lm(Y~.,data=as.data.frame(A))
L
# Check details
summary(L)
# build the X matrix (Y = XB + U)
X = cbind(rep(1,50),A)
X
B=cbind(A[,1],A[,2],A[,4])
LB=lm(Y~.,data=as.data.frame(B))
LB
summary(LB)
plot(L)
hist(L$residuals,freq = FALSE)
# Generate multiple variables
A=matrix(0,nrow=500,ncol=4)
A[,1]=rexp(500,0.4)
A[,2]=rnorm(500,3,0.5)
A[,3]=rpois(500,0.8)
A[,4]=runif(500,-3,3)
# and a response
Y=3+2*A[,1]-5*A[,2]+7*A[,4]+rnorm(500)
# Model
L=lm(Y~.,data=as.data.frame(A))
L
# Check details
summary(L)
plot(L)
hist(L$residuals,freq = FALSE)
# Generate multiple variables
A=matrix(0,nrow=500,ncol=4)
A[,1]=rexp(500,0.4)
A[,2]=rnorm(500,3,0.5)
A[,3]=rpois(500,0.8)
A[,4]=runif(500,-3,3)
# and a response
Y=3+2*A[,1]-5*A[,2]+7*A[,4]+rexp(500,0.5)
# Model
L=lm(Y~.,data=as.data.frame(A))
L
# Check details
summary(L)
#always verify normality of the noise !
plot(L)
hist(L$residuals,freq = FALSE)
# and a response
Y=3+2*A[,1]-5*A[,2]+7*A[,4]+rnorm(500)
L=lm(Y~.,data=as.data.frame(A))
L
# Check details
summary(L)
#always verify normality of the noise !
plot(L)
setwd("C:/MY_DATAS/MyGit/Statistical-Analysis/linear-regression")
install.packages(glmnet)
install.packages('glmnet')
library(glmnet)
help(glmnet)
help(glmnet)
help('glmnet')
help('glmnet')
help("glmnet")
x=matrix(rnorm(100*200),100,20)
y=rnorm(100)
fit1=glmnet(x,y,alpha=0)
plot(fit1)
x=matrix(rnorm(100*10),ncol=10)
y=rnorm(100)
fit1=glmnet(x,y,alpha=0)
plot(fit1)
fit2=glmnet(x,y,alpha=1)
plot(fit2)
#
print(fit2)
fit2cv = cv.glmnet(x,y)
plot(fit2cv)
names(fit2cv)
#
fit2cv$lambda.1se
# best value of lambda (cross validation error smallest)
fit2cv$lambda.min
plot(x,y)
plot(x)
plot(y)
fit2=glmnet(x,y,alpha=1,lambda=fit2cv$lambda.1se)
plot(fit2)
fit2
coefficients(fit2)
# general linear model
install.packages('glmnet')
library(glmnet)
help("glmnet")
# alpha = 0 --> Ridge regression
x=matrix(rnorm(100*200),100,20)
y=rnorm(100)
fit1=glmnet(x,y,alpha=0)
plot(fit1)
x=matrix(rnorm(100*10),ncol=10)
y=rnorm(100)
fit1=glmnet(x,y,alpha=0)
# trajectories of different coefficient
plot(fit1)
#Ridge regression is not a way to perform variable selection
#Lasso
# allow from a given lambda to select variables
fit2=glmnet(x,y,alpha=1)
plot(fit2)
# Df = degree of freedom (nb of variable)
print(fit2)
# cross validatio (cv.glmnet)
fit2cv = cv.glmnet(x,y)
names(fit2cv)
# best value of lambda (cross validation error smallest)
fit2cv$lambda.min
# lambda with lse (std error)
fit2cv$lambda.1se
# using cross validation lambda
fit2=glmnet(x,y,alpha=1,lambda=fit2cv$lambda.1se)
fit2
coefficients(fit2)
# general linear model
install.packages('glmnet')
library(glmnet)
help("glmnet")
# alpha = 0 --> Ridge regression
x=matrix(rnorm(100*200),100,20)
y=rnorm(100)
fit1=glmnet(x,y,alpha=0)
plot(fit1)
x=matrix(rnorm(100*10),ncol=10)
y=rnorm(100)
fit1=glmnet(x,y,alpha=0)
# trajectories of different coefficient
plot(fit1)
#Ridge regression is not a way to perform variable selection
#Lasso
# allow from a given lambda to select variables
fit2=glmnet(x,y,alpha=1)
plot(fit2)
# Df = degree of freedom (nb of variable)
print(fit2)
# cross validatio (cv.glmnet)
fit2cv = cv.glmnet(x,y)
names(fit2cv)
# best value of lambda (cross validation error smallest)
fit2cv$lambda.min
# lambda with lse (std error)
fit2cv$lambda.1se
# using cross validation lambda
fit2=glmnet(x,y,alpha=1,lambda=fit2cv$lambda.1se)
fit2
coefficients(fit2)
install.packages("glmnet")
# general linear model
# install.packages('glmnet')
library(glmnet)
# general linear model
# install.packages('glmnet')
library(glmnet)
help("glmnet")
# alpha = 0 --> Ridge regression
x=matrix(rnorm(100*200),100,20)
y=rnorm(100)
fit1=glmnet(x,y,alpha=0)
plot(fit1)
x=matrix(rnorm(100*10),ncol=10)
y=rnorm(100)
fit1=glmnet(x,y,alpha=0)
# trajectories of different coefficient
plot(fit1)
#Ridge regression is not a way to perform variable selection
#Lasso
# allow from a given lambda to select variables
fit2=glmnet(x,y,alpha=1)
plot(fit2)
# Df = degree of freedom (nb of variable)
print(fit2)
# cross validatio (cv.glmnet)
fit2cv = cv.glmnet(x,y)
names(fit2cv)
# best value of lambda (cross validation error smallest)
fit2cv$lambda.min
# lambda with lse (std error)
fit2cv$lambda.1se
# using cross validation lambda
fit2=glmnet(x,y,alpha=1,lambda=fit2cv$lambda.1se)
fit2
coefficients(fit2)
# general linear model
# install.packages('glmnet')
library(glmnet)
help("glmnet")
# alpha = 0 --> Ridge regression
x=matrix(rnorm(100*200),100,20)
y=rnorm(100)
fit1=glmnet(x,y,alpha=0)
plot(fit1)
x=matrix(rnorm(100*10),ncol=10)
y=rnorm(100)
fit1=glmnet(x,y,alpha=0)
# trajectories of different coefficient
plot(fit1)
#Ridge regression is not a way to perform variable selection
#Lasso
# allow from a given lambda to select variables
fit2=glmnet(x,y,alpha=1)
plot(fit2)
# Df = degree of freedom (nb of variable)
print(fit2)
# cross validatio (cv.glmnet)
fit2cv = cv.glmnet(x,y)
names(fit2cv)
# best value of lambda (cross validation error smallest)
fit2cv$lambda.min
# lambda with lse (std error)
fit2cv$lambda.1se
# using cross validation lambda
fit2=glmnet(x,y,alpha=1,lambda=fit2cv$lambda.1se)
fit2
coefficients(fit2)
X=martix(0,ncol=5,nrow=100)
X[,1]=runif(100,-4,3)
X[,2]=rexp(100,4)
X[,3]=rexp(100,0.02)
X[,4]=rpois(100,10)
X[,50=rf(100,2,5)]
Y=-2+3*X[,2]-5*X[,1+X[,4]]+rnorm(100)
fitcv = cv.glmnet(X,Y)
plot(fitcv)
X=matrix(0,ncol=5,nrow=100)
X[,1]=runif(100,-4,3)
X[,2]=rexp(100,4)
X[,3]=rexp(100,0.02)
X[,4]=rpois(100,10)
X[,50=rf(100,2,5)]
Y=-2+3*X[,2]-5*X[,1+X[,4]]+rnorm(100)
X=matrix(0,ncol=5,nrow=100)
X[,1]=runif(100,-4,3)
X[,2]=rexp(100,4)
X[,3]=rexp(100,0.02)
X[,4]=rpois(100,10)
X[,5]=rf(100,2,5)
Y=-2+3*X[,2]-5*X[,1+X[,4]]+rnorm(100)
X=matrix(0,ncol=5,nrow=100)
X[,1]=runif(100,-4,3)
X[,2]=rexp(100,4)
X[,3]=rexp(100,0.02)
X[,4]=rpois(100,10)
X[,5]=rf(100,2,5)
Y=-2+3*X[,2]-5*X[,1]+X[,4]+rnorm(100)
fitcv = cv.glmnet(X,Y)
plot(fitcv)
coefficients(fit)
fit = glmnet(X,Y,lambda = fitcv$lambda.1se)
coefficients(fit)
cor(X)
fitcv.lambda.1se
fitcv = cv.glmnet(X,Y)
plot(fitcv)
fitcv.lambda.1se
fit = glmnet(X,Y,lambda = fitcv$lambda.1se)
coefficients(fit)
cor(X)
fitcv = cv.glmnet(X,Y)
plot(fitcv)
fitcv.lambda.1se
fitcv$lambda.1se
names(fit2cv)
reg = lm(Y~.,data=as.data.frame(X[,1:4]))
coefficients(fit)
reg = lm(Y~.,data=as.data.frame(X[,c(1,2,4,5)]))
reg
X=matrix(0,ncol=5,nrow=100)
X[,1]=runif(100,-4,3)
X[,2]=rexp(100,4)
X[,3]=rexp(100,0.02)
X[,4]=rpois(100,10)
X[,5]=rf(100,2,5)
Y=-2+3*X[,2]-5*X[,1]+X[,4]+rnorm(100)
fitcv = cv.glmnet(X,Y)
plot(fitcv)
fitcv$lambda.1se
fit = glmnet(X,Y,lambda = fitcv$lambda.1se)
coefficients(fit)
reg = lm(Y~.,data=as.data.frame(X[,c(1,2,4)]))
reg
X=matrix(data=c(rep(1,100),
c(rep(1,20),rep(0,80)),
c(rep(0,20),rep(1,25),rep(0,55)),
c(rep(0,45),rep(1,20),rep(0,35)),
c(rep(0,65),rep(1,10),rep(0,25)),
c(rep(0,75),rep(1,25))))
X
X=matrix(data=c(rep(1,100),
c(rep(1,20),rep(0,80)),
c(rep(0,20),rep(1,25),rep(0,55)),
c(rep(0,45),rep(1,20),rep(0,35)),
c(rep(0,65),rep(1,10),rep(0,25)),
c(rep(0,75),rep(1,25))),ncol=5)
X
X=matrix(data=c(rep(1,100),
c(rep(1,20),rep(0,80)),
c(rep(0,20),rep(1,25),rep(0,55)),
c(rep(0,45),rep(1,20),rep(0,35)),
c(rep(0,65),rep(1,10),rep(0,25)),
c(rep(0,75),rep(1,25))),ncol=6)
X
B = matrix(data = c(4,0.5,0.4,0.6,.2,.1),ncol=1)
Y= X%*%B + matrix(data = rnorm(100),ncol=1)
xf = factor(rep('A',20),rep('B',25),rep('C',20),rep('D',10),rep('E',25))
xf = factor(c(rep('A',20),rep('B',25),rep('C',20),rep('D',10),rep('E',25)))
X=matrix(data=c(rep(1,100),
c(rep(1,20),rep(0,80)),
c(rep(0,20),rep(1,25),rep(0,55)),
c(rep(0,45),rep(1,20),rep(0,35)),
c(rep(0,65),rep(1,10),rep(0,25)),
c(rep(0,75),rep(1,25))),ncol=6)
B = matrix(data = c(4,0.5,0.4,0.6,.2,.1),ncol=1)
Y= X%*%B + matrix(data = rnorm(100),ncol=1)
xf = factor(c(rep('A',20),rep('B',25),rep('C',20),rep('D',10),rep('E',25)))
boxplot(Y~xf)
B = matrix(data = c(4,5,-2,7,.2,.1),ncol=1)
Y= X%*%B + matrix(data = rnorm(100),ncol=1)
xf = factor(c(rep('A',20),rep('B',25),rep('C',20),rep('D',10),rep('E',25)))
boxplot(Y~xf)
mod=lm(Y~xf)
# use lm with factor (not vector)
mod=lm(Y~xf)
mod
n1 = sum(xf=='A')
muhat = 1/n1*sum(Y*(xf=='A'))
summary(mod)
abline(mod$coefficients)
abline(mod$coefficients$xfB)
abline(mod$coefficients.xfB)
# vizualization
boxplot(Y~xf)
abline(mod$coefficients.xfB)
mod$coefficients.xfB
anova(mod)
X=matrix(data=c(rep(1,10),
c(rep(1,3),rep(0,7)),
c(rep(0,3),rep(1,3),rep(0,4)),
c(rep(0,6),rep(1,4)),
c(1,1,0,1,0,0,1,1,0,0),
c(0,0,1,0,1,1,0,0,1,1),
c(1,1,0,0,0,0,0,0,0,0),
c(0,0,1,0,0,0,0,0,0,0),
c(0,0,0,1,0,0,0,0,0,0),
c(0,0,0,0,1,1,0,0,0,0),
c(0,0,0,0,0,0,1,1,0,0),
c(0,0,0,0,0,0,0,0,1,1)),
,ncol=12)
X
X=matrix(data=c(rep(1,10),
c(rep(1,3),rep(0,7)),
c(rep(0,3),rep(1,3),rep(0,4)),
c(rep(0,6),rep(1,4)),
c(1,1,0,1,0,0,1,1,0,0),
c(0,0,1,0,1,1,0,0,1,1),
c(1,1,0,0,0,0,0,0,0,0),
c(0,0,1,0,0,0,0,0,0,0),
c(0,0,0,1,0,0,0,0,0,0),
c(0,0,0,0,1,1,0,0,0,0),
c(0,0,0,0,0,0,1,1,0,0),
c(0,0,0,0,0,0,0,0,1,1))
,ncol=12)
X
B = matrix(data = c(1,2,3,4,2,2,1,2,1,3,1,2))
B = matrix(data = c(1,2,3,4,2,2,1,2,1,3,1,2),ncol=1)
Y = X %*% B + rnorm(10)
F1 = factor(c(rep(1,3),rep(2,3),rep(3,4)))
F2 = factor(c(1,1,2,1,2,2,1,1,2,2))
mod = lm(Y~F1+F2)
# with cross effect
mod = lm(Y~F1+F2+F1*F2)
# just using additive model (no cross effect)
mod1 = lm(Y~F1+F2)
mod1
mod2
# with cross effect
mod2 = lm(Y~F1+F2+F1*F2)
mod2
boxplot(mod2)
boxplot(Y~F1+F2)
boxplot(Y~F1+F2+F1*F2)
anova(mod1)
anova(mod2)
plot(mod2)
View(B)
View(mod)
