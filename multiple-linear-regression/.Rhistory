}
Bagging <- function(X,Y,nbtree=5,nbobs=10){
# generate nb trees
for (k in 1:nbtree) {
rows_selected = sample(1:nrow(X),nobs,replace = TRUE)
newX = X[rows_selected,]
newY = Y[rows_selected]
Treek = BestTree(newX,newY)
}
}
data(mtcars)
X = mtcars[,-1]
Y = mtcars[,1]
bagtree = list()
for (k in 1:5) {
rows_selected = sample(1:nrow(X),10,replace = TRUE)
newX = X[rows_selected,]
newY = Y[rows_selected]
Treek = BestTree(newX,newY)
bagtree = list(bagtree,Treek)
}
BestTree <- function(X,Y){
# package about CART
library(rpart)
# generate max tree
TreeMAX = rpart(Y~.,data=X,control = rpart.control(cp=10^-9,minsplit=2))
# find cp corresponding to OneSE
CPs=TreeMAX$cptable
smallest_xerror = which(CPs[,'xerror'] == min(CPs[,'xerror']))
oneSE = CPs[smallest_xerror,'xerror']+CPs[smallest_xerror,'xstd']
CPBestTree = head(CPs[CPs[,'xerror']<=oneSE,],1)[,'CP']
# Prune Tree and return best tree
BestTree = prune(TreeMAX,cp=CPBestTree)
}
Bagging <- function(X,Y,nbtree=5,nbobs=10){
# generate nb trees
for (k in 1:nbtree) {
rows_selected = sample(1:nrow(X),nobs,replace = TRUE)
newX = X[rows_selected,]
newY = Y[rows_selected]
Treek = BestTree(newX,newY)
}
}
data(mtcars)
X = mtcars[,-1]
Y = mtcars[,1]
bagtree = list()
for (k in 1:5) {
rows_selected = sample(1:nrow(X),30,replace = TRUE)
newX = X[rows_selected,]
newY = Y[rows_selected]
Treek = BestTree(newX,newY)
bagtree = list(bagtree,Treek)
}
BestTree <- function(X,Y){
# package about CART
library(rpart)
# generate max tree
TreeMAX = rpart(Y~.,data=X,control = rpart.control(cp=10^-9,minsplit=2))
# find cp corresponding to OneSE
CPs=TreeMAX$cptable
smallest_xerror = which(CPs[,'xerror'] == min(CPs[,'xerror']))
oneSE = CPs[smallest_xerror,'xerror']+CPs[smallest_xerror,'xstd']
CPBestTree = head(CPs[CPs[,'xerror']<=oneSE,],1)[,'CP']
# Prune Tree and return best tree
BestTree = prune(TreeMAX,cp=CPBestTree)
}
Bagging <- function(X,Y,nbtree=5,nbobs=10){
# generate nb trees
for (k in 1:nbtree) {
rows_selected = sample(1:nrow(X),nobs,replace = TRUE)
newX = X[rows_selected,]
newY = Y[rows_selected]
Treek = BestTree(newX,newY)
}
}
data(mtcars)
X = mtcars[,-1]
Y = mtcars[,1]
bagtree = list()
for (k in 1:5) {
rows_selected = sample(1:nrow(X),30,replace = TRUE)
newX = X[rows_selected,]
newY = Y[rows_selected]
Treek = BestTree(newX,newY)
bagtree = list.append(bagtree,Treek)
}
BestTree <- function(X,Y){
# package about CART
library(rpart)
# generate max tree
TreeMAX = rpart(Y~.,data=X,control = rpart.control(cp=10^-9,minsplit=2))
# find cp corresponding to OneSE
CPs=TreeMAX$cptable
smallest_xerror = which(CPs[,'xerror'] == min(CPs[,'xerror']))
oneSE = CPs[smallest_xerror,'xerror']+CPs[smallest_xerror,'xstd']
CPBestTree = head(CPs[CPs[,'xerror']<=oneSE,],1)[,'CP']
# Prune Tree and return best tree
BestTree = prune(TreeMAX,cp=CPBestTree)
}
Bagging <- function(X,Y,nbtree=5,nbobs=10){
# generate nb trees
for (k in 1:nbtree) {
rows_selected = sample(1:nrow(X),nobs,replace = TRUE)
newX = X[rows_selected,]
newY = Y[rows_selected]
Treek = BestTree(newX,newY)
}
}
data(mtcars)
X = mtcars[,-1]
Y = mtcars[,1]
bagtree = list()
for (k in 1:5) {
rows_selected = sample(1:nrow(X),30,replace = TRUE)
newX = X[rows_selected,]
newY = Y[rows_selected]
Treek = BestTree(newX,newY)
bagtree = c(bagtree,Treek)
}
View(bagtree)
View(bagtree)
bagtree
bagtree[1]
bagtree[[1]
]
BestTree <- function(X,Y){
# package about CART
library(rpart)
# generate max tree
TreeMAX = rpart(Y~.,data=X,control = rpart.control(cp=10^-9,minsplit=2))
# find cp corresponding to OneSE
CPs=TreeMAX$cptable
smallest_xerror = which(CPs[,'xerror'] == min(CPs[,'xerror']))
oneSE = CPs[smallest_xerror,'xerror']+CPs[smallest_xerror,'xstd']
CPBestTree = head(CPs[CPs[,'xerror']<=oneSE,],1)[,'CP']
# Prune Tree and return best tree
BestTree = prune(TreeMAX,cp=CPBestTree)
}
Bagging <- function(X,Y,nbtree=5,nbobs=10){
# generate nb trees
for (k in 1:nbtree) {
rows_selected = sample(1:nrow(X),nobs,replace = TRUE)
newX = X[rows_selected,]
newY = Y[rows_selected]
Treek = BestTree(newX,newY)
}
}
data(mtcars)
X = mtcars[,-1]
Y = mtcars[,1]
bagtree = list()
for (k in 1:5) {
rows_selected = sample(1:nrow(X),30,replace = TRUE)
newX = X[rows_selected,]
newY = Y[rows_selected]
Treek = BestTree(newX,newY)
bagtree[[k]] = Treek
}
View(bagtree)
View(bagtree)
bagtree[[â™ 1]]
bagtree[[1]]
bagtree[[2]]
BestTree <- function(X,Y){
# package about CART
library(rpart)
# generate max tree
TreeMAX = rpart(Y~.,data=X,control = rpart.control(cp=10^-9,minsplit=2))
# find cp corresponding to OneSE
CPs=TreeMAX$cptable
smallest_xerror = which(CPs[,'xerror'] == min(CPs[,'xerror']))
oneSE = CPs[smallest_xerror,'xerror']+CPs[smallest_xerror,'xstd']
CPBestTree = head(CPs[CPs[,'xerror']<=oneSE,],1)[,'CP']
# Prune Tree and return best tree
BestTree = prune(TreeMAX,cp=CPBestTree)
}
Bagging <- function(X,Y,nbtree=5,nbobs=10){
# generate nb trees
for (k in 1:nbtree) {
rows_selected = sample(1:nrow(X),nobs,replace = TRUE)
newX = X[rows_selected,]
newY = Y[rows_selected]
Treek = BestTree(newX,newY)
}
}
data(mtcars)
X = mtcars[,-1]
Y = mtcars[,1]
bagtree = list()
for (k in 1:6) {
rows_selected = sample(1:nrow(X),30,replace = TRUE)
newX = X[rows_selected,]
newY = Y[rows_selected]
Treek = BestTree(newX,newY)
bagtree[[k]] = c("Tree"+k,Treek)
}
for (k in 1:6) {
print("Trre"+k)
}
for (k in 1:6) {
rows_selected = sample(1:nrow(X),30,replace = TRUE)
newX = X[rows_selected,]
newY = Y[rows_selected]
Treek = BestTree(newX,newY)
bagtree[[k]] = c(paste("Tree",k),Treek)
}
View(bagtree)
bagtree = list()
for (k in 1:6) {
rows_selected = sample(1:nrow(X),30,replace = TRUE)
newX = X[rows_selected,]
newY = Y[rows_selected]
Treek = BestTree(newX,newY)
bagtree[[k]] = c(paste("Tree",k),Treek)
}
View(bagtree)
View(bagtree)
bagtree
bagtree[[1]]
bagtree[[1]]
bagtree[[k]] = c(Treek)
bagtree = list()
for (k in 1:6) {
rows_selected = sample(1:nrow(X),30,replace = TRUE)
newX = X[rows_selected,]
newY = Y[rows_selected]
Treek = BestTree(newX,newY)
bagtree[[k]] = c(Treek)
}
bagtree = list()
for (k in 1:6) {
rows_selected = sample(1:nrow(X),30,replace = TRUE)
newX = X[rows_selected,]
newY = Y[rows_selected]
Treek = BestTree(newX,newY)
bagtree[[k]] = Treek
}
bagtree[[1]]
bagtree = list()
for (k in 1:6) {
rows_selected = sample(1:nrow(X),30,replace = TRUE)
newX = X[rows_selected,]
newY = Y[rows_selected]
Treek = BestTree(newX,newY)
bagtree[[k]] = c(paste("Tree",k),Treek)
}
bagtree['Tree1']
bagtree[['Tree1']]
bagtree$Tree1
bagtree = list()
for (k in 1:6) {
rows_selected = sample(1:nrow(X),30,replace = TRUE)
newX = X[rows_selected,]
newY = Y[rows_selected]
Treek = BestTree(newX,newY)
bagtree[[k]] = c(paste("Tree",k),Treek)
}
bagtree$Tree1
View(bagtree)
bagtree = list()
for (k in 1:6) {
rows_selected = sample(1:nrow(X),30,replace = TRUE)
newX = X[rows_selected,]
newY = Y[rows_selected]
Treek = BestTree(newX,newY)
bagtree = c(bagtree,Treek)
}
bagtree = list()
for (k in 1:5) {
rows_selected = sample(1:nrow(X),30,replace = TRUE)
newX = X[rows_selected,]
newY = Y[rows_selected]
Treek = BestTree(newX,newY)
bagtree = c(bagtree,Treek)
}
bagtree = list()
for (k in 1:5) {
rows_selected = sample(1:nrow(X),30,replace = TRUE)
newX = X[rows_selected,]
newY = Y[rows_selected]
Treek = BestTree(newX,newY)
bagtree = append(bagtree,list(Treek))
}
View(bagtree)
bagtree[[1]]
bagtree[1]
View(bagtree)
View(bagtree)
bagtree = list()
for (k in 1:5) {
rows_selected = sample(1:nrow(X),30,replace = TRUE)
newX = X[rows_selected,]
newY = Y[rows_selected]
Treek = BestTree(newX,newY)
bagtree = append(bagtree,list(Treek))
names(bagtree)[k] <- paste("Tree",k)
}
View(bagtree)
BestTree <- function(X,Y){
# package about CART
library(rpart)
# generate max tree
TreeMAX = rpart(Y~.,data=X,control = rpart.control(cp=10^-9,minsplit=2))
# find cp corresponding to OneSE
CPs=TreeMAX$cptable
smallest_xerror = which(CPs[,'xerror'] == min(CPs[,'xerror']))
oneSE = CPs[smallest_xerror,'xerror']+CPs[smallest_xerror,'xstd']
CPBestTree = head(CPs[CPs[,'xerror']<=oneSE,],1)[,'CP']
# Prune Tree and return best tree
BestTree = prune(TreeMAX,cp=CPBestTree)
}
Bagging <- function(X,Y,nbtree=5,nbobs=30){
# generate nb trees
for (k in 1:nbtree) {
rows_selected = sample(1:nrow(X),nobs,replace = TRUE)
newX = X[rows_selected,]
newY = Y[rows_selected]
Treek = BestTree(newX,newY)
bagtree = append(bagtree,list(Treek))
names(bagtree)[k] <- paste("Tree",k)
}
Bagging <- bagtree
}
data(mtcars)
X = mtcars[,-1]
Y = mtcars[,1]
Trees = Bagging(X,Y,nbtree=5,nobs=30)
Trees = Bagging(X,Y,nbtree=5,nbobs=30)
Bagging <- function(X,Y,nbtree=5,nbobs=30){
# generate nb trees
for (k in 1:nbtree) {
rows_selected = sample(1:nrow(X),nbobs,replace = TRUE)
newX = X[rows_selected,]
newY = Y[rows_selected]
Treek = BestTree(newX,newY)
bagtree = append(bagtree,list(Treek))
names(bagtree)[k] <- paste("Tree",k)
}
Bagging <- bagtree
}
Trees = Bagging(X,Y,nbtree=5,nbobs=30)
Bagging <- function(X,Y,nbtree=5,nbobs=30){
bagtree = list()
# generate nb trees
for (k in 1:nbtree) {
rows_selected = sample(1:nrow(X),nbobs,replace = TRUE)
newX = X[rows_selected,]
newY = Y[rows_selected]
Treek = BestTree(newX,newY)
bagtree = append(bagtree,list(Treek))
names(bagtree)[k] <- paste("Tree",k)
}
Bagging <- bagtree
}
Trees = Bagging(X,Y,nbtree=5,nbobs=30)
View(Trees)
Trees = Bagging(X,Y,nbtree=10,nbobs=30)
View(bagtree)
help("predict")
View(X)
newobs=as.list(c(disp=183,hp=175,drat=3.45,wt=2.42,qsec=18.5,vs=1,am=0,gear=3,carb=2))
result = 0
for (k in 1:nrow(Trees)) {
result = result + 1/k *predict(Trees[[k]], newdata=newobs)
}
result = 0
for (k in 1:length(Trees)) {
result = result + 1/k *predict(Trees[[k]], newdata=newobs)
}
predict(Trees[[k]], newdata=newobs)
predict(Trees[[1]], newdata=newobs,)
predict(Trees[[1]], newdata=newobs)
Trees[[1]]
predict(Trees[1], newdata=newobs)
predict(Trees[[1]], newdata=newobs)
setwd("C:/MY_DATAS/MyGit/Statistical-Analysis/cart-randomforest")
install.packages('xgboost')
# install.packages('xgboost')
library(xgboost)
help("xgboost")
# install.packages('xgboost')
library(xgboost)
data(agaricus.train, package='xgboost')
data(agaricus.test, package='xgboost')
dtrain <- xgb.DMatrix(agaricus.train$data, label = agaricus.train$label)
dtest <- xgb.DMatrix(agaricus.test$data, label = agaricus.test$label)
watchlist <- list(train = dtrain, eval = dtest)
## A simple xgb.train example:
param <- list(max_depth = 2, eta = 1, verbose = 0, nthread = 2,
objective = "binary:logistic", eval_metric = "auc")
bst <- xgb.train(param, dtrain, nrounds = 2, watchlist)
## An xgb.train example where custom objective and evaluation metric are used:
logregobj <- function(preds, dtrain) {
labels <- getinfo(dtrain, "label")
preds <- 1/(1 + exp(-preds))
grad <- preds - labels
hess <- preds * (1 - preds)
return(list(grad = grad, hess = hess))
}
evalerror <- function(preds, dtrain) {
labels <- getinfo(dtrain, "label")
err <- as.numeric(sum(labels != (preds > 0)))/length(labels)
return(list(metric = "error", value = err))
}
# These functions could be used by passing them either:
#  as 'objective' and 'eval_metric' parameters in the params list:
param <- list(max_depth = 2, eta = 1, verbose = 0, nthread = 2,
objective = logregobj, eval_metric = evalerror)
bst <- xgb.train(param, dtrain, nrounds = 2, watchlist)
#  or through the ... arguments:
param <- list(max_depth = 2, eta = 1, verbose = 0, nthread = 2)
bst <- xgb.train(param, dtrain, nrounds = 2, watchlist,
objective = logregobj, eval_metric = evalerror)
#  or as dedicated 'obj' and 'feval' parameters of xgb.train:
bst <- xgb.train(param, dtrain, nrounds = 2, watchlist,
obj = logregobj, feval = evalerror)
## An xgb.train example of using variable learning rates at each iteration:
param <- list(max_depth = 2, eta = 1, verbose = 0, nthread = 2,
objective = "binary:logistic", eval_metric = "auc")
my_etas <- list(eta = c(0.5, 0.1))
bst <- xgb.train(param, dtrain, nrounds = 2, watchlist,
callbacks = list(cb.reset.parameters(my_etas)))
## Early stopping:
bst <- xgb.train(param, dtrain, nrounds = 25, watchlist,
early_stopping_rounds = 3)
## An 'xgboost' interface example:
bst <- xgboost(data = agaricus.train$data, label = agaricus.train$label,
max_depth = 2, eta = 1, nthread = 2, nrounds = 2,
objective = "binary:logistic")
pred <- predict(bst, agaricus.test$data)
res = predict(bst,dtest)
# install.packages('xgboost')
library(xgboost)
data(agaricus.train, package='xgboost')
data(agaricus.test, package='xgboost')
dtrain <- xgb.DMatrix(agaricus.train$data, label = agaricus.train$label)
dtest <- xgb.DMatrix(agaricus.test$data, label = agaricus.test$label)
watchlist <- list(train = dtrain, eval = dtest)
## A simple xgb.train example:
param <- list(max_depth = 2, eta = 1, verbose = 0, nthread = 2,
objective = "binary:logistic", eval_metric = "auc")
bst <- xgb.train(param, dtrain, nrounds = 2, watchlist)
res = predict(bst,dtest)
# ## An xgb.train example where custom objective and evaluation metric are used:
# logregobj <- function(preds, dtrain) {
#   labels <- getinfo(dtrain, "label")
#   preds <- 1/(1 + exp(-preds))
#   grad <- preds - labels
#   hess <- preds * (1 - preds)
#   return(list(grad = grad, hess = hess))
# }
# evalerror <- function(preds, dtrain) {
#   labels <- getinfo(dtrain, "label")
#   err <- as.numeric(sum(labels != (preds > 0)))/length(labels)
#   return(list(metric = "error", value = err))
# }
#
# # These functions could be used by passing them either:
# #  as 'objective' and 'eval_metric' parameters in the params list:
# param <- list(max_depth = 2, eta = 1, verbose = 0, nthread = 2,
#               objective = logregobj, eval_metric = evalerror)
# bst <- xgb.train(param, dtrain, nrounds = 2, watchlist)
#
# #  or through the ... arguments:
# param <- list(max_depth = 2, eta = 1, verbose = 0, nthread = 2)
# bst <- xgb.train(param, dtrain, nrounds = 2, watchlist,
#                  objective = logregobj, eval_metric = evalerror)
#
# #  or as dedicated 'obj' and 'feval' parameters of xgb.train:
# bst <- xgb.train(param, dtrain, nrounds = 2, watchlist,
#                  obj = logregobj, feval = evalerror)
#
#
# ## An xgb.train example of using variable learning rates at each iteration:
# param <- list(max_depth = 2, eta = 1, verbose = 0, nthread = 2,
#               objective = "binary:logistic", eval_metric = "auc")
# my_etas <- list(eta = c(0.5, 0.1))
# bst <- xgb.train(param, dtrain, nrounds = 2, watchlist,
#                  callbacks = list(cb.reset.parameters(my_etas)))
#
# ## Early stopping:
# bst <- xgb.train(param, dtrain, nrounds = 25, watchlist,
#                  early_stopping_rounds = 3)
#
# ## An 'xgboost' interface example:
# bst <- xgboost(data = agaricus.train$data, label = agaricus.train$label,
#                max_depth = 2, eta = 1, nthread = 2, nrounds = 2,
#                objective = "binary:logistic")
# pred <- predict(bst, agaricus.test$data)
res
setwd("C:/MY_DATAS/MyGit/Statistical-Analysis/linear-regression")
ozone = read.table('ozone.txt')
setwd("C:/MY_DATAS/MyGit/Statistical-Analysis/multiple-linear-regression")
ozone = read.table('ozone.txt')
ozone = read.table('ozone.txt')
X1 = ozone[,1,12]
X1 = ozone[,c(1,12)]
summary(X1)
plot(maxO3~vent,data=X1)
fit = lm(maxO3~vent,data=X1)
anova(fit)
fit
summary(fit)
# 1-way anova
X2 = ozone[,c(1,12,13)]
summary(X2)
plot(maxO3~vent*pluie,data=X2)
fit = lm(maxO3~vent*pluie,data=X2)
fit2 = lm(maxO3~vent*pluie,data=X2)
summary(fit2)
anova(fit2)
